{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032e562a-15fc-4767-bd5e-556e667418f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [0.0.0.0]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [worker01]\n",
      "Starting namenodes on [0.0.0.0]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [worker01]\n",
      "214373 DataNode\n",
      "214555 SecondaryNameNode\n",
      "212008 SparkSubmit\n",
      "214680 Jps\n",
      "214189 NameNode\n",
      "Safe mode is ON\n",
      "Safe mode is OFF\n",
      "Dataset URL: https://www.kaggle.com/datasets/robertvici/indonesia-top-ecommerce-unicorn-tweets\n",
      "License(s): copyright-authors\n",
      "indonesia-top-ecommerce-unicorn-tweets.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  /home/hduser/kaggle-datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "  inflating: /home/hduser/kaggle-datasets/ShopeeID.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/bliblidotcom.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/bukalapak.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/lazadaID.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/tokopedia.json  \n",
      "put: `e-commerce/datasets/bliblidotcom.json': File exists\n",
      "put: `e-commerce/datasets/bukalapak.json': File exists\n",
      "put: `e-commerce/datasets/lazadaID.json': File exists\n",
      "put: `e-commerce/datasets/ShopeeID.json': File exists\n",
      "put: `e-commerce/datasets/tokopedia.json': File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 20:10:55.202831: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-20 20:10:55.215962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734700255.231838  213677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734700255.236460  213677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 20:10:55.252407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/20 20:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/20 20:11:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/12/20 20:11:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 20:11:25.554704: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 845035.1875 - mae: 31.6108\n",
      "Epoch 2/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 927009.8750 - mae: 33.0268\n",
      "Epoch 3/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 968650.6250 - mae: 29.3704\n",
      "Epoch 4/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 853620.5625 - mae: 26.7506\n",
      "Epoch 5/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 658006.9375 - mae: 26.8828\n",
      "Epoch 6/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 539886.8125 - mae: 24.9783\n",
      "Epoch 7/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 514703.7188 - mae: 27.4117\n",
      "Epoch 8/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 1187194.3750 - mae: 34.2074\n",
      "Epoch 9/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 918055.8125 - mae: 31.6637\n",
      "Epoch 10/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 838902.5000 - mae: 30.5814\n",
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_model/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140520993110736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140520993111312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140520993111888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140520994029840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140520994030992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, count\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "def run_command(command):\n",
    "    \"\"\"Utility function to run shell commands\"\"\"\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode == 0:\n",
    "        print(f\"Success: {command}\\nOutput:\\n{stdout.decode()}\")\n",
    "    else:\n",
    "        print(f\"Error: {command}\\nError Message:\\n{stderr.decode()}\")\n",
    "\n",
    "# Restart DFS\n",
    "print(\"Stopping DFS...\")\n",
    "run_command(\"stop-dfs.sh\")\n",
    "\n",
    "print(\"Starting DFS...\")\n",
    "run_command(\"start-dfs.sh\")\n",
    "\n",
    "# Check running Java services\n",
    "print(\"Checking Java processes...\")\n",
    "run_command(\"jps\")\n",
    "\n",
    "# Check Safe Mode status\n",
    "print(\"Checking Safe Mode status...\")\n",
    "run_command(\"hdfs dfsadmin -safemode get\")\n",
    "\n",
    "# Leave Safe Mode if it's ON\n",
    "print(\"Leaving Safe Mode if necessary...\")\n",
    "run_command(\"hdfs dfsadmin -safemode leave\")\n",
    "\n",
    "# Create HDFS directories\n",
    "hdfs_dirs = [\n",
    "    \"e-commerce/datasets\",\n",
    "    \"e-commerce/splits\",\n",
    "    \"e-commerce/models\",\n",
    "    \"e-commerce/outputs\"\n",
    "]\n",
    "\n",
    "for hdfs_dir in hdfs_dirs:\n",
    "    print(f\"Creating HDFS directory: {hdfs_dir}\")\n",
    "    run_command(f\"hdfs dfs -mkdir -p {hdfs_dir}\")\n",
    "\n",
    "print(\"All tasks completed successfully!\")\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "kaggle_dataset_path = \"~/kaggle-datasets\"\n",
    "dataset_name = \"indonesia-top-ecommerce-unicorn-tweets\"\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "run_command(f\"kaggle datasets download -d robertvici/{dataset_name} -p {kaggle_dataset_path}\")\n",
    "\n",
    "# Unzip the downloaded dataset\n",
    "zip_file_path = f\"{kaggle_dataset_path}/{dataset_name}.zip\"\n",
    "print(\"Unzipping dataset...\")\n",
    "run_command(f\"unzip -o {zip_file_path} -d {kaggle_dataset_path}\")\n",
    "\n",
    "# Upload files to HDFS\n",
    "print(\"Uploading JSON files to HDFS...\")\n",
    "run_command(f\"hdfs dfs -put {kaggle_dataset_path}/*.json e-commerce/datasets/\")\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Engagement Prediction ML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "blibli_df = spark.read.json('e-commerce/datasets/bliblidotcom.json')\n",
    "bukalapak_df = spark.read.json('e-commerce/datasets/bukalapak.json')\n",
    "lazadaID_df = spark.read.json('e-commerce/datasets/lazadaID.json')\n",
    "shopeeID_df = spark.read.json('e-commerce/datasets/ShopeeID.json')\n",
    "tokopedia_df = spark.read.json('e-commerce/datasets/tokopedia.json')\n",
    "\n",
    "# Add a new column to identify the company source\n",
    "blibli_df = blibli_df.withColumn('source', lit('blibli'))\n",
    "bukalapak_df = bukalapak_df.withColumn('source', lit('bukalapak'))\n",
    "lazadaID_df = lazadaID_df.withColumn('source', lit('lazadaID'))\n",
    "shopeeID_df = shopeeID_df.withColumn('source', lit('shopeeID'))\n",
    "tokopedia_df = tokopedia_df.withColumn('source', lit('tokopedia'))\n",
    "\n",
    "# Merge datasets using union (axis=0 equivalent in Spark)\n",
    "merged_df = blibli_df.union(bukalapak_df).union(lazadaID_df).union(shopeeID_df).union(tokopedia_df)\n",
    "\n",
    "# Clean tweet text\n",
    "def clean_text(text):\n",
    "    return text.lower().replace(\"#\", \"\").strip()\n",
    "\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply text cleaning and create new features\n",
    "data_cleaned = merged_df.withColumn(\"clean_tweet\", clean_text_udf(col(\"tweet\"))) \\\n",
    "                       .withColumn(\"engagement\", col(\"replies_count\") + col(\"retweets_count\") + col(\"likes_count\"))\n",
    "\n",
    "# Select relevant features\n",
    "selected_data = data_cleaned.select(\n",
    "    col(\"clean_tweet\").alias(\"text\"),\n",
    "    col(\"replies_count\").alias(\"replies\"),\n",
    "    col(\"retweets_count\").alias(\"retweets\"),\n",
    "    col(\"likes_count\").alias(\"likes\"),\n",
    "    col(\"engagement\").alias(\"target\"),\n",
    "    col(\"hashtags\"),    \n",
    "    col(\"source\")\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "train_data, validate_data, test_data = selected_data.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Save splits for later use\n",
    "train_data.write.json(\"e-commerce/splits/train.json\", mode=\"overwrite\")\n",
    "validate_data.write.json(\"commerce/splits/validate.json\", mode=\"overwrite\")\n",
    "test_data.write.json(\"commerce/splits/test.json\", mode=\"overwrite\")\n",
    "\n",
    "# Change null value with 0 (if any)\n",
    "merged_df = merged_df.fillna({\"likes_count\": 0, \"replies_count\": 0, \"retweets_count\": 0})\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check negative value\n",
    "merged_df.filter((F.col(\"likes_count\") < 0) | (F.col(\"replies_count\") < 0) | (F.col(\"retweets_count\") < 0)).show()\n",
    "\n",
    "# Change negative value with 0 (if any)\n",
    "for col in [\"likes_count\", \"replies_count\", \"retweets_count\"]:\n",
    "    merged_df = merged_df.withColumn(col, F.when(F.col(col) < 0, 0).otherwise(F.col(col)))\n",
    "    \n",
    "# Matching target engagement definitions in Spark DataFrame\n",
    "blibli_df = blibli_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "bukalapak_df = bukalapak_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "lazadaID_df = lazadaID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "shopeeID_df = shopeeID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "tokopedia_df = tokopedia_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "\n",
    "\n",
    "# Load train data (convert Spark DataFrame to Pandas)\n",
    "train_df = train_data.toPandas()\n",
    "\n",
    "# Tokenize and vectorize text (fit on original text, not the padded sequences)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # Fit tokenizer on the raw text data\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
    "\n",
    "y_train = np.array(train_df[\"target\"])\n",
    "\n",
    "\n",
    "# Define a simple Neural Network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "vocab_size = 42500  # Customize with your tokenizer\n",
    "embedding_dim = 128\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "# Example: Tokenize and pad the input text\n",
    "max_vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # 'train_df[\"text\"]' should be a list of strings\n",
    "\n",
    "# Example of saving the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "X_train = pad_sequences(X_train_sequences, padding='post')\n",
    "\n",
    "# Ensure y_train is in the correct format (e.g., a numpy array)\n",
    "y_train = np.array(train_df[\"target\"])  # Adjust this based on your target column\n",
    "\n",
    "# Train the model \n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model with a valid file extension in local server\n",
    "model.save(\"e-commerce-engagement_model.keras\")  # For the native Keras format\n",
    "\n",
    "# Export to save model\n",
    "model.export(\"saved_model/1\")\n",
    "print(\"Final Model ==> saved_model/1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
