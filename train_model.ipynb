{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467ba864-47fd-4474-a5e8-6876f9cb6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Stopping nodemanagers\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "localhost: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n",
      "Stopping resourcemanager\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Stopping namenodes on [0.0.0.0]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [worker01]\n"
     ]
    }
   ],
   "source": [
    "# Stop yarn and dfs\n",
    "!stop-yarn.sh\n",
    "!stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52604116-85c1-4e2d-8681-37ab20aa56de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [0.0.0.0]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [worker01]\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Starting resourcemanager\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Starting nodemanagers\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n"
     ]
    }
   ],
   "source": [
    "# Restart dfs and yarn\n",
    "!start-dfs.sh\n",
    "!start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329ea11a-0272-48e0-bb3e-bbcd4c4d0087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43666 NameNode\n",
      "44006 SecondaryNameNode\n",
      "44343 NodeManager\n",
      "43817 DataNode\n",
      "44687 Jps\n"
     ]
    }
   ],
   "source": [
    "# Check java services running\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f58d9f7-4985-4fb6-b8be-b797b43de30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is ON\n"
     ]
    }
   ],
   "source": [
    "# Check Safe mode: should be OFF\n",
    "!hdfs dfsadmin -safemode get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555bacff-8e91-436b-ad61-efa4f3c5488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\n"
     ]
    }
   ],
   "source": [
    "# If Safe mode is ON --> execute hdfs dfsadmin -safemode leave\n",
    "!hdfs dfsadmin -safemode leave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510b2629-66eb-43c8-b1bc-d349480adb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:31 e-commerce\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-10-10 21:11 output\n",
      "-rw-r--r--   3 hduser supergroup      12331 2024-10-26 10:57 taxi_zone_lookup.csv\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:44 test.json\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:44 train.json\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:44 validate.json\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-11-05 09:20 warehouse\n"
     ]
    }
   ],
   "source": [
    "# Check hdfs home files\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43adfed-98fb-40ae-9445-d07ed0c69c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hdfs directories\n",
    "!hdfs dfs -mkdir -p \\\n",
    "    e-commerce/datasets \\\n",
    "    e-commerce/splits \\\n",
    "    e-commerce/models \\\n",
    "    e-commerce/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbe113f-0f26-4604-b6ca-a72e98590548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:44 e-commerce/datasets\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:27 e-commerce/models\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:29 e-commerce/outputs\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-12-19 08:27 e-commerce/splits\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls e-commerce/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049f49ba-9f7d-47a2-b430-f9182a647ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/robertvici/indonesia-top-ecommerce-unicorn-tweets\n",
      "License(s): copyright-authors\n",
      "indonesia-top-ecommerce-unicorn-tweets.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  /home/hduser/kaggle-datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "  inflating: /home/hduser/kaggle-datasets/ShopeeID.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/bliblidotcom.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/bukalapak.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/lazadaID.json  \n",
      "  inflating: /home/hduser/kaggle-datasets/tokopedia.json  \n",
      "put: `e-commerce/datasets/bliblidotcom.json': File exists\n",
      "put: `e-commerce/datasets/bukalapak.json': File exists\n",
      "put: `e-commerce/datasets/lazadaID.json': File exists\n",
      "put: `e-commerce/datasets/ShopeeID.json': File exists\n",
      "put: `e-commerce/datasets/tokopedia.json': File exists\n"
     ]
    }
   ],
   "source": [
    "# Download kaggle dataset into hdfs\n",
    "\n",
    "!kaggle datasets download -d robertvici/indonesia-top-ecommerce-unicorn-tweets -p ~/kaggle-datasets\n",
    "!unzip -o ~/kaggle-datasets/indonesia-top-ecommerce-unicorn-tweets.zip -d ~/kaggle-datasets\n",
    "!hdfs dfs -put ~/kaggle-datasets/*.json e-commerce/datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec040fbd-4aee-4764-90ba-03fcba4790ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/user/hduser/ecommerce/datasets': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Show list of dataset files downloaded\n",
    "!hdfs dfs -ls /user/hduser/ecommerce/datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b1de9a-ac0e-456c-91e1-102e8122efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 08:55:24.925342: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 08:55:24.939970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734573324.956688   42933 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734573324.961771   42933 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-19 08:55:24.979109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, count\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff51c0d7-24d7-4684-8405-d9b338a0e535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/19 08:55:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Engagement Prediction ML\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28422e13-eb93-4304-b5aa-330dacd1ab40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 08:55:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/12/19 08:55:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+---------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "|cashtags|    conversation_id|   created_at|      date|geo|            hashtags|                 id|likes_count|                link|         mentions|      name|near|              photos|place|quote_url|replies_count|            reply_to|retweet|retweet_date|retweet_id|retweets_count|   source|    time|timezone|trans_dest|trans_src|translate|               tweet|                urls|  user_id|user_rt|user_rt_id|    username|video|\n",
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+---------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "|      []|1256116141040242689|1588316400000|2020-05-01|   | [#ramadanlebihbaik]|1256116141040242689|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|bukalapak|17:00:00|    AEST|          |         |         |Sudah kepikiran m...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256101043999436802|1588312800000|2020-05-01|   | [#ramadanlebihbaik]|1256101043999436802|          0|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|bukalapak|16:00:00|    AEST|          |         |         |Belanja pake kart...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256085943099981825|1588309200000|2020-05-01|   |[#ramadan, #ramad...|1256085943099981825|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|bukalapak|15:00:00|    AEST|          |         |         |Udah seminggu #Ra...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308276000|2020-05-01|   |                  []|1256082069408505856|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|bukalapak|14:44:36|    AEST|          |         |         |Catet tanggal mai...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308276000|2020-05-01|   |    [#ngobrolbareng]|1256082066355007488|          1|https://twitter.c...|               []|Blibli.com|    |[https://pbs.twim...|     |         |            1|[{280834900, blib...|  false|            |          |             1|bukalapak|14:44:36|    AEST|          |         |         |Yuk! Gabung di #N...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308275000|2020-05-01|   |                  []|1256082063796498432|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            1|[{280834900, blib...|  false|            |          |             2|bukalapak|14:44:35|    AEST|          |         |         |Udah bosen banget...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256065886688931841|1588304418000|2020-05-01|   |       [#dirumahaja]|1256065886688931841|          3|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            1|[{280834900, blib...|  false|            |          |             1|bukalapak|13:40:18|    AEST|          |         |         |Long weekend! Bak...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256060570828062720|1588303151000|2020-05-01|   |       [#mayday2020]|1256060570828062720|          3|https://twitter.c...|               []|Blibli.com|    |[https://pbs.twim...|     |         |            0|[{280834900, blib...|  false|            |          |             0|bukalapak|13:19:11|    AEST|          |         |         |SelamatÂ HariÂ Buru...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255509152165593088|1588302916000|2020-05-01|   |                  []|1256059585124659200|          0|https://twitter.c...|[surabayaheroes7]|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|bukalapak|13:15:16|    AEST|          |         |         |Cek di sini, ya h...|[https://www.blib...|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255867000049373185|1588257000000|2020-05-01|   |[#blogbliblifriends]|1255867000049373185|          2|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|bukalapak|00:30:00|    AEST|          |         |         |Buang semua racun...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+---------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"eCommerce Data\").getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "blibli_df = spark.read.json('e-commerce/datasets/bliblidotcom.json')\n",
    "bukalapak_df = spark.read.json('e-commerce/datasets/bukalapak.json')\n",
    "lazadaID_df = spark.read.json('e-commerce/datasets/lazadaID.json')\n",
    "shopeeID_df = spark.read.json('e-commerce/datasets/ShopeeID.json')\n",
    "tokopedia_df = spark.read.json('e-commerce/datasets/tokopedia.json')\n",
    "\n",
    "# Add a new column to identify the company source\n",
    "blibli_df = blibli_df.withColumn('source', lit('bukalapak'))\n",
    "bukalapak_df = bukalapak_df.withColumn('source', lit('bukalapak'))\n",
    "lazadaID_df = lazadaID_df.withColumn('source', lit('bukalapak'))\n",
    "shopeeID_df = shopeeID_df.withColumn('source', lit('shopee'))\n",
    "tokopedia_df = tokopedia_df.withColumn('source', lit('tokopedia'))\n",
    "\n",
    "# Merge datasets using union (axis=0 equivalent in Spark)\n",
    "merged_df = blibli_df.union(bukalapak_df).union(lazadaID_df).union(shopeeID_df).union(tokopedia_df)\n",
    "\n",
    "# Show merged data\n",
    "merged_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17c61c38-6632-4594-9c74-100224ed0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the merged DataFrame:\n",
      "root\n",
      " |-- cashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- conversation_id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- likes_count: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- mentions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- near: string (nullable = true)\n",
      " |-- photos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- quote_url: string (nullable = true)\n",
      " |-- replies_count: long (nullable = true)\n",
      " |-- reply_to: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- user_id: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |-- retweet: boolean (nullable = true)\n",
      " |-- retweet_date: string (nullable = true)\n",
      " |-- retweet_id: string (nullable = true)\n",
      " |-- retweets_count: long (nullable = true)\n",
      " |-- source: string (nullable = false)\n",
      " |-- time: string (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- trans_dest: string (nullable = true)\n",
      " |-- trans_src: string (nullable = true)\n",
      " |-- translate: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_rt: string (nullable = true)\n",
      " |-- user_rt_id: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- video: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame structure\n",
    "print(\"Schema of the merged DataFrame:\")\n",
    "merged_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a05cab-6fa7-4545-86c4-5de54133d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the merged DataFrame: 541180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "row_count = merged_df.count()\n",
    "print(f\"Number of rows in the merged DataFrame: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f83160-3283-4c27-9827-105cd79cf95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+-----+------+--------------------+---------+\n",
      "|                text|replies|retweets|likes|target|            hashtags|   source|\n",
      "+--------------------+-------+--------+-----+------+--------------------+---------+\n",
      "|sudah kepikiran m...|      0|       0|    1|     1| [#ramadanlebihbaik]|bukalapak|\n",
      "|belanja pake kart...|      0|       0|    0|     0| [#ramadanlebihbaik]|bukalapak|\n",
      "|udah seminggu ram...|      0|       0|    1|     1|[#ramadan, #ramad...|bukalapak|\n",
      "|catet tanggal mai...|      0|       1|    1|     2|                  []|bukalapak|\n",
      "|yuk! gabung di ng...|      1|       1|    1|     3|    [#ngobrolbareng]|bukalapak|\n",
      "|udah bosen banget...|      1|       2|    1|     4|                  []|bukalapak|\n",
      "|long weekend! bak...|      1|       1|    3|     5|       [#dirumahaja]|bukalapak|\n",
      "|selamatÂ hariÂ buru...|      0|       0|    3|     3|       [#mayday2020]|bukalapak|\n",
      "|cek di sini, ya h...|      0|       0|    0|     0|                  []|bukalapak|\n",
      "|buang semua racun...|      0|       1|    2|     3|[#blogbliblifriends]|bukalapak|\n",
      "|kalau aku;\\n\\nker...|      0|       1|    3|     4|[#blogbliblifriends]|bukalapak|\n",
      "|biar ramadanlebih...|      0|       0|    1|     1|[#ramadanlebihbai...|bukalapak|\n",
      "|biar wfh jadi mak...|      0|       0|    2|     2| [#ramadanlebihbaik]|bukalapak|\n",
      "|hayo, selama diru...|      0|       1|    1|     2|[#dirumahaja, #bl...|bukalapak|\n",
      "|terbaru! printed ...|      0|       1|    1|     2|[#maskeruntukindo...|bukalapak|\n",
      "|              ðŸ˜‹ðŸ˜‹ðŸ˜‹|      0|       0|    0|     0|                  []|bukalapak|\n",
      "|buat kamu yang se...|      0|       0|    3|     3|[#dirumahaja, #ra...|bukalapak|\n",
      "|mau belanja kebut...|      0|       0|    0|     0|    [#karenakamuno1]|bukalapak|\n",
      "|selamat berbuka p...|      0|       0|    3|     3| [#ramadanlebihbaik]|bukalapak|\n",
      "|ramadanlebihbaik ...|      0|       0|    2|     2| [#ramadanlebihbaik]|bukalapak|\n",
      "+--------------------+-------+--------+-----+------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean tweet text\n",
    "def clean_text(text):\n",
    "    return text.lower().replace(\"#\", \"\").strip()\n",
    "\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply text cleaning and create new features\n",
    "data_cleaned = merged_df.withColumn(\"clean_tweet\", clean_text_udf(col(\"tweet\"))) \\\n",
    "                       .withColumn(\"engagement\", col(\"replies_count\") + col(\"retweets_count\") + col(\"likes_count\"))\n",
    "\n",
    "# Select relevant features\n",
    "selected_data = data_cleaned.select(\n",
    "    col(\"clean_tweet\").alias(\"text\"),\n",
    "    col(\"replies_count\").alias(\"replies\"),\n",
    "    col(\"retweets_count\").alias(\"retweets\"),\n",
    "    col(\"likes_count\").alias(\"likes\"),\n",
    "    col(\"engagement\").alias(\"target\"),\n",
    "    col(\"hashtags\"),    \n",
    "    col(\"source\")\n",
    ")\n",
    "\n",
    "# Show processed data\n",
    "selected_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d7f6bb-82f9-4152-9972-d244aced7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_data, validate_data, test_data = selected_data.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Save splits for later use\n",
    "train_data.write.json(\"e-commerce/splits/train.json\", mode=\"overwrite\")\n",
    "validate_data.write.json(\"commerce/splits/validate.json\", mode=\"overwrite\")\n",
    "test_data.write.json(\"commerce/splits/test.json\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e29f860-9aa1-4628-b8ca-91f075b460fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "\n",
    "merged_df.filter(\n",
    "    (merged_df.likes_count.isNull()) | \n",
    "    (merged_df.replies_count.isNull()) | \n",
    "    (merged_df.retweets_count.isNull())\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "914beadf-8663-40e4-a083-cece2aa5f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change null value with 0 (if any)\n",
    "merged_df = merged_df.fillna({\"likes_count\": 0, \"replies_count\": 0, \"retweets_count\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faeef860-a909-4ce0-afba-c83ec9175ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 08:55:45 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check negative value\n",
    "merged_df.filter((F.col(\"likes_count\") < 0) | (F.col(\"replies_count\") < 0) | (F.col(\"retweets_count\") < 0)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de9ee086-8cc5-4f6c-bc9e-0e3a58d451bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change negative value with 0 (if any)\n",
    "for col in [\"likes_count\", \"replies_count\", \"retweets_count\"]:\n",
    "    merged_df = merged_df.withColumn(col, F.when(F.col(col) < 0, 0).otherwise(F.col(col)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60dd39fc-37b0-47f2-9592-33e388b13bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching target engagement definitions in Spark DataFrame\n",
    "blibli_df = blibli_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "bukalapak_df = bukalapak_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "lazadaID_df = lazadaID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "shopeeID_df = shopeeID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "tokopedia_df = tokopedia_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc44acd8-b16c-4031-9daa-592901231add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Load train data (convert Spark DataFrame to Pandas)\n",
    "train_df = train_data.toPandas()\n",
    "\n",
    "# Tokenize and vectorize text (fit on original text, not the padded sequences)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # Fit tokenizer on the raw text data\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
    "\n",
    "y_train = np.array(train_df[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "660eacea-5c60-4f8b-8631-c7ac0c4ae681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Neural Network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a6c46e9-340d-4297-8a02-836a810ede82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 08:55:57.327175: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d22e52-f469-4541-baf5-ba422592fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 42500  # Customize with your tokenizer\n",
    "embedding_dim = 128\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a39659-e862-47ab-a4f7-d1df11637ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 814507.5000 - mae: 30.1946\n",
      "Epoch 2/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 1376708.3750 - mae: 35.0661\n",
      "Epoch 3/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 818767.5000 - mae: 27.2493\n",
      "Epoch 4/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 502845.1875 - mae: 24.4029\n",
      "Epoch 5/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 515621.8750 - mae: 25.4223\n",
      "Epoch 6/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 769788.1250 - mae: 29.2789\n",
      "Epoch 7/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 698755.2500 - mae: 26.9724\n",
      "Epoch 8/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 708657.6250 - mae: 27.6812\n",
      "Epoch 9/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 770464.6875 - mae: 28.6672\n",
      "Epoch 10/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 332138.1875 - mae: 25.3133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2b353bda50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Tokenize and pad the input text\n",
    "max_vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # 'train_df[\"text\"]' should be a list of strings\n",
    "\n",
    "# Example of saving the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "X_train = pad_sequences(X_train_sequences, padding='post')\n",
    "\n",
    "# Ensure y_train is in the correct format (e.g., a numpy array)\n",
    "y_train = np.array(train_df[\"target\"])  # Adjust this based on your target column\n",
    "\n",
    "# Train the model !!!\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f405028-7ce6-442f-a83b-ac159f661646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with a valid file extension in local server\n",
    "model.save(\"e-commerce-engagement_model.keras\")  # For the native Keras format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaaa9962-5bb0-4c83-9527-9a955b9740fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  139823553030544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139823553031120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139823553031888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139823553031312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139823553033232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Export to save model\n",
    "model.export(\"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65a767-f8ce-43a4-bd5a-49e860e1ff84",
   "metadata": {},
   "source": [
    "## API Gateway with Flask/FastAPI\n",
    "\n",
    "Create endpoint to call TensorFlow Serving API:\n",
    "\n",
    "1. pip install flask requests\n",
    "\n",
    "2. Create predict_model.py\n",
    "   \n",
    "   ```\n",
    "   from flask import Flask, request, jsonify\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    # Initialize Flask app\n",
    "    app = Flask(__name__)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    with open('tokenizer.pkl', 'rb') as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "    \n",
    "    model = load_model('engagement_model.keras')\n",
    "    \n",
    "    # Set a maximum sequence length (should match what was used during training)\n",
    "    MAX_SEQUENCE_LENGTH = 20\n",
    "    \n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict():\n",
    "        data = request.get_json()\n",
    "        input_text = data.get('text', '')\n",
    "    \n",
    "        if not input_text:\n",
    "            return jsonify({'error': 'No text provided'}), 400\n",
    "    \n",
    "        try:\n",
    "            # Convert text to sequences\n",
    "            sequences = tokenizer.texts_to_sequences([input_text])\n",
    "            \n",
    "            # Pad the sequences to ensure consistent input shape\n",
    "            padded_input = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "            # Convert to NumPy array (Keras requires this)\n",
    "            processed_input = np.array(padded_input)\n",
    "    \n",
    "            # Perform prediction\n",
    "            prediction = model.predict(processed_input)\n",
    "    \n",
    "            # Return prediction\n",
    "            return jsonify({'prediction': prediction.tolist()})\n",
    "    \n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    \n",
    "   if __name__ == \"__main__\":\n",
    "        app.run(host='0.0.0.0', port=5001)\n",
    "\n",
    "    ```\n",
    "\n",
    "3. Run API\n",
    "\n",
    "    ```\n",
    "    python predict_model.py\n",
    "   \n",
    "    ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fa167-7bb9-4a34-91c5-531482005582",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ee4a40a-9312-4b1b-9517-7a15d50941ed",
   "metadata": {},
   "source": [
    "# Run on terminal\n",
    "# python predict_model.py\n",
    "\n",
    "* Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:5000\n",
    " * Running on http://192.168.241.128:5000\n",
    "Press CTRL+C to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3cc9e5-9e02-4e0a-a1f0-d75b844f5c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[[23.862144470214844]]}\n"
     ]
    }
   ],
   "source": [
    "# Use cURL or Postman for testing:\n",
    "!curl -X POST -H \"Content-Type: application/json\" -d '{\"text\": \"Hadiah langsung\"}' http://localhost:5000/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d2b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[[23.862144470214844]]}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -d '{\"text\": \"Hadiah langsung\"}' http://192.168.241.128:5000/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee7a47-56bb-4d12-b924-db9ae370ddef",
   "metadata": {},
   "source": [
    "## Features & Target\n",
    "\n",
    "- Features:- \n",
    "    - clean_tweet: Preprocessed tweet text.\n",
    "    - hashtags: The number or presence of hashtags.\n",
    "    - replies_count\n",
    "    - retweets_count\n",
    "    - likes_count\n",
    "      \n",
    "- Target:\n",
    "    - engagement: A combination of likes, replies, and retweets.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee06c2e-e087-4ac8-9251-571ed5394150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
